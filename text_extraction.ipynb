{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb48453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber \n",
    "import re \n",
    "import json \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976a1a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANUSANTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ANUSANTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea76733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text extraction\n",
    "def read_pdf(file_path):\n",
    "    text = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "#preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \",text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    clauses = sent_tokenize(text)\n",
    "    return clauses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52778d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clauses_in_batch(clauses):\n",
    "    \n",
    "    \n",
    "    model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
    "    \n",
    "    \n",
    "    clauses_with_ids = [{\"id\": i, \"text\": clause} for i, clause in enumerate(clauses)]\n",
    "    \n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert legal risk analyst. You will be given a JSON array of legal clauses, each with a unique \"id\". \n",
    "    Your task is to analyze every clause and return a single JSON array as your response. \n",
    "    Each object in your returned array must correspond to a clause from the input and contain:\n",
    "    1. \"id\": The original ID of the clause.\n",
    "    2. \"analysis\": An object containing your analysis with the following four fields:\n",
    "        - \"risky\": A boolean (true/false).\n",
    "        - \"score\": An integer from 0 (no risk) to 100 (critical risk).\n",
    "        - \"summary\": A concise, one-sentence summary of the clause's meaning.\n",
    "        - \"reason\": A concise, one-sentence explanation.\n",
    "        - \"category\": One of ['Financial', 'Liability', 'Operational', 'Compliance', 'Termination', 'Data Privacy', 'Intellectual Property', 'Uncategorized'].\n",
    "\n",
    "    Process all clauses provided in the input JSON and respond ONLY with the resulting JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"{system_prompt}\\n\\nCLAUSES_JSON:\\n{json.dumps(clauses_with_ids, indent=2)}\"\n",
    "    \n",
    "    print(f\"Sending {len(clauses)} clauses to the AI...\")\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        json_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        analyses = json.loads(json_text)\n",
    "        \n",
    "        analysis_map = {item['id']: item['analysis'] for item in analyses}\n",
    "        \n",
    "        final_results = []\n",
    "        for clause_data in clauses_with_ids:\n",
    "            clause_id = clause_data['id']\n",
    "            analysis = analysis_map.get(clause_id, {\n",
    "                \"risky\": False, \"score\": 0, \"summary\": \"Analysis not found in batch response.\", \"reason\": \"Analysis not found in batch response.\", \"category\": \"Error\"\n",
    "            })\n",
    "            final_results.append({\n",
    "                \"Clause\": clause_data['text'],\n",
    "                **analysis\n",
    "            })\n",
    "        return final_results\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        print(f\"--> CRITICAL ERROR: Could not parse the batch AI response. Error: {e}\")\n",
    "        # Return an error for all clauses if the batch fails\n",
    "        return [{\"Clause\": clause, \"risky\": False, \"score\": 0, \"reason\": \"Batch analysis failed.\", \"category\": \"Error\"} for clause in clauses]\n",
    "    except Exception as e:\n",
    "        print(f\"--> An unexpected API error occurred: {e}\")\n",
    "        return [{\"Clause\": clause, \"risky\": False, \"score\": 0, \"reason\": \"API error.\", \"category\": \"Error\"} for clause in clauses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe422e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_risk(analysis_results):\n",
    "    \n",
    "    risky_clauses = [r for r in analysis_results if r.get('risky')]\n",
    "    if not risky_clauses:\n",
    "        return {\n",
    "            \"overall_risk_score\": 0,\n",
    "            \"risk_summary\": \"No significant risks were identified in this document.\",\n",
    "            \"total_clauses\": len(analysis_results),\n",
    "            \"risky_clause_count\": 0\n",
    "        }\n",
    "\n",
    "    \n",
    "    average_score = sum(c['score'] for c in risky_clauses) / len(risky_clauses)\n",
    "    \n",
    "    \n",
    "    top_risks = sorted(risky_clauses, key=lambda x: x['score'], reverse=True)[:3]\n",
    "    risk_summary_points = [f\"- {c['summary']} (Risk Score: {c['score']})\" for c in top_risks]\n",
    "    risk_summary = \"The primary risks identified are:\\n\" + \"\\n\".join(risk_summary_points)\n",
    "\n",
    "    return {\n",
    "        \"overall_risk_score\": round(average_score),\n",
    "        \"risk_summary\": risk_summary,\n",
    "        \"total_clauses\": len(analysis_results),\n",
    "        \"risky_clause_count\": len(risky_clauses)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb67002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json export\n",
    "\n",
    "def export_final_json(document_summary, clause_analysis, output_file=\"analysis.json\"):\n",
    "    \"\"\"\n",
    "    Exports the final JSON with the document summary at the top level.\n",
    "    \"\"\"\n",
    "    final_output = {\n",
    "        \"document_summary\": document_summary,\n",
    "        \"clause_by_clause_analysis\": clause_analysis\n",
    "    }\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "054790eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized analysis of samples\\Hostel_Damage_Policy.pdf...\n",
      "Sending 10 clauses to the AI...\n",
      "\n",
      "Analysis complete. The file 'analysis.json' has been saved!\n"
     ]
    }
   ],
   "source": [
    "# --- Main Function ---\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "\n",
    "pdf_file = r\"samples\\Hostel_Damage_Policy.pdf\"\n",
    "\n",
    "\n",
    "print(f\"Starting optimized analysis of {pdf_file}...\")\n",
    "raw_text = read_pdf(pdf_file)\n",
    "clauses = preprocess_text(raw_text)\n",
    " \n",
    "clause_analysis = analyze_clauses_in_batch(clauses) \n",
    "\n",
    "\n",
    "document_summary = calculate_overall_risk(clause_analysis)\n",
    "\n",
    "\n",
    "export_final_json(document_summary, clause_analysis)\n",
    "\n",
    "print(f\"\\nAnalysis complete. The file 'analysis.json' has been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d227000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
