{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb48453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber \n",
    "import re \n",
    "import json \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976a1a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANUSANTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ANUSANTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea76733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text extraction\n",
    "def read_pdf(file_path):\n",
    "    text = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    The definitive function to parse documents. It is resilient to line breaks\n",
    "    within a single sentence and uses a more precise filtering logic.\n",
    "    \"\"\"\n",
    "   \n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
    "    \n",
    "    text = re.sub(r'\\n(?!\\s*(?:\\d+\\.|\\([a-z\\d]+\\)|[A-Z\\s]{5,}))', ' ', text)\n",
    "    \n",
    "    blocks = text.split('\\n')\n",
    "    \n",
    "    final_clauses = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        block = block.strip()\n",
    "        \n",
    "        if not block:\n",
    "            continue\n",
    "        if len(block.split()) < 3 and block.endswith(':'):\n",
    "            continue\n",
    "        if re.match(r'^(Lessor|Lessee|Date:)', block, re.IGNORECASE):\n",
    "            continue\n",
    "        \n",
    "        # Skip serial numbers like \"1.\", \"1.1\", \"(a)\", \"(i)\"\n",
    "        block = re.sub(r'^\\s*(\\d+(\\.\\d+)*\\.|\\([a-zA-Z\\d]+\\))\\s*', '', block)\n",
    "\n",
    "        sentences = sent_tokenize(block)\n",
    "        final_clauses.extend(sentences)\n",
    "        \n",
    "    return final_clauses\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52778d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clauses_in_batch(clauses):\n",
    "    \n",
    "    \n",
    "    model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
    "    \n",
    "    \n",
    "    clauses_with_ids = [{\"id\": i, \"text\": clause} for i, clause in enumerate(clauses)]\n",
    "    \n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert legal risk analyst. YOUR CLIENT IS THE PERSON SIGNING THE DOCUMENT (e.g., the Lessee or Occupant). \n",
    "    Your entire analysis must be from THEIR PERSPECTIVE, identifying risks that could negatively affect them. You will be given a JSON array of legal clauses, each with a unique \"id\". \n",
    "    Your task is to analyze every clause and return a single JSON array as your response. \n",
    "    Each object in your returned array must correspond to a clause from the input and contain:\n",
    "    1. \"id\": The original ID of the clause.\n",
    "    2. \"analysis\": An object containing your analysis with the following four fields:\n",
    "        - \"risky\": A boolean (true/false).\n",
    "        - \"score\": An integer from 0 (no risk) to 100 (critical risk).\n",
    "        - \"summary\": A concise, one-sentence summary of the clause's meaning.\n",
    "        - \"reason\": A concise, one-sentence explanation.\n",
    "        - \"category\": One of ['Financial', 'Liability', 'Operational', 'Compliance', 'Termination', 'Data Privacy', 'Intellectual Property', 'Uncategorized'].\n",
    "\n",
    "    Process all clauses provided in the input JSON and respond ONLY with the resulting JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"{system_prompt}\\n\\nCLAUSES_JSON:\\n{json.dumps(clauses_with_ids, indent=2)}\"\n",
    "    \n",
    "    print(f\"Sending {len(clauses)} clauses to the AI...\")\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        json_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        analyses = json.loads(json_text)\n",
    "        \n",
    "        analysis_map = {item['id']: item['analysis'] for item in analyses}\n",
    "        \n",
    "        final_results = []\n",
    "        for clause_data in clauses_with_ids:\n",
    "            clause_id = clause_data['id']\n",
    "            analysis = analysis_map.get(clause_id, {\n",
    "                \"risky\": False, \"score\": 0, \"summary\": \"Analysis not found in batch response.\", \"reason\": \"Analysis not found in batch response.\", \"category\": \"Error\"\n",
    "            })\n",
    "            final_results.append({\n",
    "                \"Clause\": clause_data['text'],\n",
    "                **analysis\n",
    "            })\n",
    "        return final_results\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        print(f\"--> CRITICAL ERROR: Could not parse the batch AI response. Error: {e}\")\n",
    "        # Return an error for all clauses if the batch fails\n",
    "        return [{\"Clause\": clause, \"risky\": False, \"score\": 0, \"reason\": \"Batch analysis failed.\", \"category\": \"Error\"} for clause in clauses]\n",
    "    except Exception as e:\n",
    "        print(f\"--> An unexpected API error occurred: {e}\")\n",
    "        return [{\"Clause\": clause, \"risky\": False, \"score\": 0, \"reason\": \"API error.\", \"category\": \"Error\"} for clause in clauses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d929c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe422e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_risk(analysis_results):\n",
    "    \n",
    "    risky_clauses = [r for r in analysis_results if r.get('risky')]\n",
    "    if not risky_clauses:\n",
    "        return {\n",
    "            \"overall_risk_score\": 0,\n",
    "            \"risk_summary\": \"No significant risks were identified in this document.\",\n",
    "            \"total_clauses\": len(analysis_results),\n",
    "            \"risky_clause_count\": 0\n",
    "        }\n",
    "\n",
    "    \n",
    "    average_score = sum(c['score'] for c in risky_clauses) / len(risky_clauses)\n",
    "    \n",
    "    \n",
    "    top_risks = sorted(risky_clauses, key=lambda x: x['score'], reverse=True)[:3]\n",
    "    risk_summary_points = [f\"- {c['summary']} (Risk Score: {c['score']})\" for c in top_risks]\n",
    "    risk_summary = \"The primary risks identified are:\\n\" + \"\\n\".join(risk_summary_points)\n",
    "\n",
    "    return {\n",
    "        \"overall_risk_score\": round(average_score),\n",
    "        \"risk_summary\": risk_summary,\n",
    "        \"total_clauses\": len(analysis_results),\n",
    "        \"risky_clause_count\": len(risky_clauses)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb67002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json export\n",
    "\n",
    "def export_final_json(document_summary, clause_analysis, output_file=\"analysis.json\"):\n",
    "    \"\"\"\n",
    "    Exports the final JSON with the document summary at the top level.\n",
    "    \"\"\"\n",
    "    final_output = {\n",
    "        \"document_summary\": document_summary,\n",
    "        \"clause_by_clause_analysis\": clause_analysis\n",
    "    }\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054790eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing samples\\Hostel_Rental_Rules.pdf...\n",
      "Sending 18 clauses to the AI...\n",
      "\n",
      "Analysis complete. The file 'analysis.json' has been saved!\n"
     ]
    }
   ],
   "source": [
    "# --- Main Function ---\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "pdf_file = r\"samples\\Hostel_Rental_Rules.pdf\"\n",
    "\n",
    "print(f\"Analysing {pdf_file}...\")\n",
    "raw_text = read_pdf(pdf_file)\n",
    "clauses = preprocess_text(raw_text)\n",
    " \n",
    "clause_analysis = analyze_clauses_in_batch(clauses) \n",
    "\n",
    "document_summary = calculate_overall_risk(clause_analysis)\n",
    "\n",
    "export_final_json(document_summary, clause_analysis)\n",
    "print(f\"\\nAnalysis complete. The file 'analysis.json' has been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d227000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing analyzer with document: samples\\Hostel_Rental_Rules.pdf\n",
      "Creating vector store for 18 clauses...\n",
      "Analyzer is ready.\n",
      "\n",
      "Q: What is the penalty for late payment?\n",
      "A: The penalty for late payment will be decided by management.\n",
      "\n",
      "Q: Can I have visitors stay overnight?\n",
      "A: No, visitors are not permitted to stay overnight without prior written approval from the hostel management.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chatBot\n",
    "class LegalAnalyzer:\n",
    "    def __init__(self, pdf_path):\n",
    "        \"\"\"\n",
    "        This is the one-time setup. It runs when the class is created.\n",
    "        It loads the PDF, preprocesses it, and builds the knowledge base.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing analyzer with document: {pdf_path}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"The document was not found at: {pdf_path}\")\n",
    "\n",
    "        raw_text = read_pdf(pdf_path)\n",
    "        self.clauses = preprocess_text(raw_text)\n",
    "\n",
    "        self.vector_store = self._create_vector_store()\n",
    "        print(\"Analyzer is ready.\")\n",
    "\n",
    "    def _create_vector_store(self, model='models/text-embedding-004'):\n",
    "        print(f\"Creating vector store for {len(self.clauses)} clauses...\")\n",
    "        result = genai.embed_content(model=model, content=self.clauses, task_type=\"retrieval_document\")\n",
    "        embeddings = result['embedding']\n",
    "        dimension = len(embeddings[0])\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(np.array(embeddings))\n",
    "        return index\n",
    "\n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        This is the on-demand chatbot function. It's very fast.\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            return \"Vector store is not available.\"\n",
    "\n",
    "        question_embedding = genai.embed_content(model='models/text-embedding-004', content=question, task_type=\"retrieval_query\")['embedding']\n",
    "        k = 3\n",
    "        _, indices = self.vector_store.search(np.array([question_embedding]), k)\n",
    "        \n",
    "        relevant_clauses = [self.clauses[i] for i in indices[0]]\n",
    "        context = \"\\n\".join(relevant_clauses)\n",
    "        \n",
    "        rag_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "        prompt = f\"Using ONLY the context below, answer the user's question.\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION: {question}\\n\\nANSWER:\"\n",
    "        response = rag_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in .env file.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "   \n",
    "    pdf_file_path = r\"samples\\Hostel_Rental_Rules.pdf\"\n",
    "    analyzer = LegalAnalyzer(pdf_path=pdf_file_path)\n",
    "\n",
    "    \n",
    "    question_1 = \"What is the penalty for late payment?\"\n",
    "    answer_1 = analyzer.ask_question(question_1)\n",
    "    print(f\"\\nQ: {question_1}\\nA: {answer_1}\\n\")\n",
    "\n",
    "    question_2 = \"Can I have visitors stay overnight?\"\n",
    "    answer_2 = analyzer.ask_question(question_2)\n",
    "    print(f\"Q: {question_2}\\nA: {answer_2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2ab19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
